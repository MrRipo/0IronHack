{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Hunting and Gathering (Part 1)\n",
    "\n",
    "![Web Scraping](http://unadocenade.com/wp-content/uploads/2012/09/cavalls-de-valltorta.jpg)\n",
    "\n",
    "Welcome to the first part of our journey into the world of web scraping. Web scraping, also known as web harvesting or web data extraction, is a technique used for extracting data from websites. This process involves fetching the web page and then extracting data from it.\n",
    "\n",
    "## Why Learn Web Scraping?\n",
    "Understanding how to scrape data from the web is a valuable skill for any data professional. In the digital era, data is the new gold, and web scraping is the mining equipment. Here's why it's essential:\n",
    "\n",
    "- **Data Availability**: The internet is a vast source of data for all kinds of analyses, from market trends to academic research.\n",
    "- **Automation**: Web scraping can automate the process of collecting data, saving time and effort.\n",
    "- **Competitive Advantage**: In many fields, having timely and relevant data can be a game-changer.\n",
    "\n",
    "## Real-World Applications\n",
    "- **Market Research**: Analyzing competitors, understanding customer sentiments, and identifying market trends.\n",
    "- **Price Comparison**: Aggregating pricing data from various websites for comparison shopping.\n",
    "- **Social Media Analysis**: Gathering data from social networks for sentiment analysis or trend spotting.\n",
    "\n",
    "## Ethical Considerations in Web Scraping\n",
    "\n",
    "Web scraping, while a powerful technique for data extraction, comes with significant ethical and legal responsibilities. As budding data scientists and web scrapers, it's crucial to navigate this landscape with a deep understanding and respect for these considerations.\n",
    "\n",
    "### Respecting Website Policies and Laws\n",
    "\n",
    "- **Adhering to Terms of Service**: Every website has its own set of rules, usually outlined in its Terms of Service (ToS). It's important to read and understand these rules before scraping, as violating them can have legal implications.\n",
    "\n",
    "- **Following Copyright Laws**: The data you scrape is often copyrighted. Ensure that your use of scraped data complies with copyright laws and respects intellectual property rights.\n",
    "\n",
    "- **Privacy Concerns**: Be mindful of personal data. Scraping and using personal information without consent can breach privacy laws and ethical standards.\n",
    "\n",
    "### Example: Understanding Google's `robots.txt`\n",
    "\n",
    "Google's `robots.txt` file is an excellent example of how websites communicate their scraping policies. Accessible at [Google's robots.txt](https://www.google.com/robots.txt), this file provides directives to web crawlers about which pages they can or cannot scrape.\n",
    "\n",
    "#### Implications of Google's `robots.txt`\n",
    "\n",
    "- **Selective Access**: Google allows certain parts of its site to be crawled while restricting others. For instance, crawling the search results pages is generally disallowed.\n",
    "\n",
    "- **Dynamic Nature**: The content of `robots.txt` files can change, reflecting the website's evolving stance on web scraping. Regular checks are necessary for compliance.\n",
    "\n",
    "- **Respecting the Limits**: Even if a `robots.txt` file allows scraping of some pages, it does not automatically mean all scraping activities are legally or ethically acceptable. It's a guideline, not a blanket permission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Data Hunting in the Digital Age\n",
    "\n",
    "## The Evolution of Data Sourcing\n",
    "\n",
    "In this course, we focus on data as our foundational element. Traditionally, data has been sourced from structured formats like spreadsheets from scientific experiments or records in relational databases within organizations. But with the digital revolution, particularly the advent of the internet, our approach to data collection must evolve. The internet is a vast reservoir of unstructured data, presenting both challenges and opportunities for data retrieval and analysis.\n",
    "\n",
    "## Understanding the Landscape of Web Data\n",
    "\n",
    "When seeking data from the internet, it's essential to first consider how the website in question provides access to its data. Many large-scale websites like Google, Facebook, and Twitter offer an **Application Programming Interface (API)**. APIs are designed to facilitate easy access to a website's data in a structured format, simplifying the process of data extraction.\n",
    "\n",
    "### The Role of APIs\n",
    "\n",
    "- **APIs as a Primary Tool**: An API acts as a bridge between the data seeker and the website's database, allowing for streamlined data retrieval.\n",
    "- **Limitations**: However, not all websites provide an API. Additionally, even when an API is available, it may not grant access to all the data a user might need.\n",
    "\n",
    "### The Need for Web Scraping\n",
    "\n",
    "In cases where an API is absent or insufficient, we turn to **web scraping**. Web scraping involves extracting raw data directly from a website's frontend - essentially, the same information presented to users in their web browsers.\n",
    "\n",
    "#### Diving into Scraping\n",
    "\n",
    "- **Dealing with Unstructured Data**: Scraping requires us to interact with unstructured data, necessitating custom coding and data parsing techniques.\n",
    "- **Legal and Ethical Considerations**: It's crucial to approach web scraping with an awareness of the legal and ethical implications, respecting website policies and user privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"border-radius:20px;\" src=\"./files/big_picture.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Our Journey\n",
    "\n",
    "Our first practical step in this journey will be to explore how to connect to the internet and retrieve a basic webpage. We'll begin by using Python's `urllib.request` module, a powerful tool for interacting with URLs and handling web requests.\n",
    "\n",
    "Join us as we embark on this exciting journey to master the art of data hunting in the digital era, where we'll navigate the complexities of APIs, web scraping, and the ethical considerations that come with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http.client.HTTPResponse object at 0x000001FFC97A3C10>\n"
     ]
    }
   ],
   "source": [
    "# Import the 'urlopen' function from the 'urllib.request' module.\n",
    "# This function is used for opening URLs, which is the first step in web scraping.\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Use the 'urlopen' function to open the URL 'http://www.google.com/'.\n",
    "# The function returns a response object which can be used to read the content of the page.\n",
    "# Here, 'source' is a variable that holds the response object from the URL.\n",
    "source = urlopen(\"http://www.google.com/\")\n",
    "\n",
    "# Print the response object.\n",
    "# This command does not print the content of the webpage.\n",
    "# Instead, it prints a representation of the response object, \n",
    "# which includes information like the URL, HTTP response status, headers, etc.\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet demonstrates the basic usage of the `urlopen` function for accessing a webpage. However, it is important to note that `print(source)` will not display the HTML content of the webpage but rather the HTTP response object's representation. To view the actual content of the page, you would need to read from the `source` object using methods like `source.read()`.\n",
    "\n",
    "# Exploring the Content Retrieved by `urlopen`\n",
    "\n",
    "After opening a URL using the `urlopen` function from the `urllib.request` module, we typically want to access the actual content of the webpage. This is where `source.read()` comes into play.\n",
    "\n",
    "## Understanding `source.read()`\n",
    "\n",
    "When you call `urlopen`, it returns an HTTPResponse object. This object, which we've named `source` in our example, holds various data and metadata about the webpage. To extract the actual HTML content of the page, we use the `read` method on this object.\n",
    "\n",
    "### What Does `source.read()` Do?\n",
    "\n",
    "- **Retrieves Webpage Content**: `source.read()` reads the entire content of the webpage to which the URL points. This content is usually in HTML format, which is the standard language for creating webpages.\n",
    "\n",
    "- **Binary Format**: The data retrieved is in binary format. To work with it as a string in Python, you might need to decode it using a method like `.decode('utf-8')`.\n",
    "\n",
    "- **One-time Operation**: It's important to note that you can read the content of the response only once. After `source.read()` is executed, the response object does not retain the content in a readable form. If you need to access the content again, you must reopen the URL.\n",
    "\n",
    "Here's a simple example to illustrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us check what is in\n",
    "something = source.read()\n",
    "print(something)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check type\n",
    "type(something)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm-Up Exercises\n",
    "\n",
    "Let's get our hands-on with some initial exercises to get warmed up with web scraping!\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Python.org Content Check**: Does [https://www.python.org](https://www.python.org) contain the word `Python`?  \n",
    "   _Hint: You can use the `in` keyword to check._\n",
    "\n",
    "2. **Google.com Image Search**: Does [http://google.com](http://google.com) contain an image?  \n",
    "   _Hint: Look for the `<img>` tag._\n",
    "\n",
    "3. **First Characters of Python.org**: What are the first ten characters of [https://www.python.org](https://www.python.org)?\n",
    "\n",
    "4. **Keyword Check in Pyladies.com**: Is there the word 'python' in [https://pyladies.com](https://pyladies.com)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EX1: Check if 'Python' is in the content of http://www.python.org/\n",
    "\n",
    "# Import the urlopen function from the urllib.request module\n",
    "# This function is used to open a URL and retrieve its contents\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Use the urlopen function to access the webpage at http://www.python.org/\n",
    "# The function returns an HTTPResponse object which is stored in the variable 'source'\n",
    "source = urlopen(\"http://www.python.org/\")\n",
    "\n",
    "# Read the content of the response object using the read() method\n",
    "# The read() method retrieves the content of the webpage in binary format\n",
    "# The binary content is then decoded to a string using the 'latin-1' encoding\n",
    "# The decoded string is stored in the variable 'something'\n",
    "something = source.read().decode('latin-1')\n",
    "\n",
    "# Check if the word \"Python\" is in the decoded string\n",
    "# This is done using the 'in' keyword, which checks for the presence of a substring in a string\n",
    "# The result is a boolean value: True if \"Python\" is found, False otherwise\n",
    "\"Python\" in something\n",
    "\n",
    "# Note: The choice of 'latin-1' for decoding might not always be appropriate\n",
    "# It's often better to use 'utf-8', which is a more common encoding for webpages\n",
    "# For example: something = source.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EX2: Check if 'https://www.google.com/' contains an image tag (\"<img>\")\n",
    "\n",
    "# Import the urlopen function from the urllib.request module.\n",
    "# This function is used to open a URL and retrieve its contents.\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Use the urlopen function to open the webpage at 'https://www.google.com/'.\n",
    "# The function returns an HTTPResponse object, which we store in the variable 'source'.\n",
    "source = urlopen(\"https://www.google.com/\")\n",
    "\n",
    "# Read the content of the response object using the read() method.\n",
    "# The read() method retrieves the content of the webpage in binary format.\n",
    "# After reading, the content is in bytes, which is not human-readable.\n",
    "# We then decode this binary content into a string using the 'latin-1' encoding.\n",
    "# The resulting string, which contains the HTML of the page, is stored in 'something'.\n",
    "something = source.read().decode('latin-1')\n",
    "\n",
    "# Check if the string \"img\" is in the decoded HTML content.\n",
    "# This is a simple way to check if there is an <img> tag in the HTML,\n",
    "# as \"img\" is part of the standard HTML tag for images.\n",
    "# The result will be True if \"img\" is found (indicating the presence of an image),\n",
    "# and False if not.\n",
    "\"img\" in something\n",
    "\n",
    "# Note: Decoding with 'latin-1' might not be suitable for all websites,\n",
    "# especially if the website uses a different character set.\n",
    "# 'utf-8' is a more commonly used encoding and is often a better choice.\n",
    "# For instance: something = source.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is your turn for EX3 and EX4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `urlopen` vs. `Request` in Web Scraping\n",
    "\n",
    "When performing web scraping tasks in Python, you have the option to use either the `urlopen` function from the `urllib.request` module or the `Request` object in combination with `urlopen`. Here, we'll explain why you might choose one approach over the other.\n",
    "\n",
    "## Using `urlopen` Directly\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "- **Simplicity**: It's a straightforward way to access a webpage and retrieve its content without the need for additional objects or customization.\n",
    "  \n",
    "- **Default Behavior**: `urlopen` uses default settings for the HTTP request, which is suitable for many common use cases.\n",
    "\n",
    "- **Convenience**: For simple web scraping tasks, it provides a concise and readable solution.\n",
    "\n",
    "## Using `Request` with `urlopen`\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "- **Customization**: You can set custom headers, use different HTTP methods (e.g., POST, PUT), and configure advanced options like handling redirects, cookies, and timeouts.\n",
    "\n",
    "- **Fine-Grained Control**: It offers greater flexibility for handling complex scenarios.\n",
    "\n",
    "In summary, the choice between using `urlopen` directly and creating a `Request` object depends on the complexity of your web scraping task. For simple tasks like fetching webpage content, `urlopen` is often sufficient and more straightforward. However, if you need to customize headers, use non-GET HTTP methods, or handle advanced scenarios, creating a `Request` object allows for fine-grained control over your HTTP requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution exercise 4\n",
    "# import urllib\n",
    "# url ='https://www.pyladies.com'\n",
    "# req = urllib.request.Request(url, headers = {'User-Agent': 'Magic Browser'})\n",
    "# con = urllib.request.urlopen(req)\n",
    "# html = con.read().decode()\n",
    "\n",
    "# 'Python' in html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling and Scraping: Unveiling the Web's Secrets\n",
    "\n",
    "Crawling and scraping are two fundamental techniques in the world of web data acquisition. They form the backbone of many data-driven applications and are crucial skills for data analysts and web developers.\n",
    "\n",
    "## Crawling: Navigating the Web\n",
    "\n",
    "Crawling, often referred to as web crawling or web scraping, is the process of systematically navigating the World Wide Web to retrieve web pages. Think of it as a web robot or spider, tirelessly traversing the internet to discover and index web content. This technique is at the heart of search engines like Google and Bing.\n",
    "\n",
    "### Why Do We Crawl?\n",
    "\n",
    "Crawling serves several important purposes:\n",
    "\n",
    "- **Indexing**: It allows search engines to index and catalog web pages, making them searchable by users.\n",
    "  \n",
    "- **Link Discovery**: Crawlers extract links from web pages, helping build a vast network of interconnected web resources. This link structure is crucial for understanding the web's architecture.\n",
    "  \n",
    "- **Data Retrieval**: Crawlers may scrape or extract data from web pages, but their primary goal is to discover and navigate to other web pages.\n",
    "\n",
    "## Scraping: Harvesting Data\n",
    "\n",
    "Scraping is the process of extracting specific data or information from a single web page. Unlike crawling, which focuses on navigating the web, scraping zooms in on a single webpage to harvest valuable data.\n",
    "\n",
    "### Use Cases of Scraping\n",
    "\n",
    "Scraping is used for a variety of purposes, such as:\n",
    "\n",
    "- **Data Extraction**: It allows us to extract structured data like product prices, news headlines, or stock market information from websites.\n",
    "\n",
    "- **Content Monitoring**: Scraping can be employed to track changes in content on specific web pages, such as monitoring price changes on e-commerce sites or tracking news updates.\n",
    "\n",
    "- **Competitor Analysis**: Businesses often use scraping to gather data on competitors, such as pricing strategies or product listings.\n",
    "\n",
    "- **Research and Analysis**: Data analysts and researchers use scraping to collect data for studies, reports, and data-driven insights.\n",
    "\n",
    "## Crawling and Scraping Synergy\n",
    "\n",
    "In practice, crawling and scraping often work together. Crawlers traverse the web to find new pages, and once they reach a page of interest, scraping techniques are applied to extract valuable data. This synergy is what powers search engines, news aggregators, and data-driven applications on the internet.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Understanding the concepts of crawling and scraping is essential for anyone looking to work with web data. Whether you want to build a search engine, gather market research, or simply automate data collection, these techniques are your gateway to unlocking the wealth of information available on the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARM-UP PROJECT: Building a Simple Web Spider**\n",
    "\n",
    "In this warm-up project, we'll delve into the world of web spiders or crawlers. These are specialized programs designed to systematically explore the World Wide Web, retrieving web pages and their contents. Web spiders play a crucial role in various applications, including indexing web pages for search engines, data extraction from websites, and more. In this project, we'll focus on constructing a basic web spider.\n",
    "\n",
    "---\n",
    "\n",
    "**Project Overview:**\n",
    "\n",
    "A web spider, also known as a web crawler, is essentially a digital agent that navigates the vast landscape of the internet. Its primary mission is to:\n",
    "\n",
    "- Explore the web by following links from one webpage to another.\n",
    "- Retrieve web page content.\n",
    "- Store valuable data for analysis or other purposes.\n",
    "\n",
    "Think of it as a robotic explorer, tirelessly traversing the web to gather information. In our project, we aim to create a simplified version of such a web spider.\n",
    "\n",
    "**Key Tasks:**\n",
    "\n",
    "1. **Identifying Links**: The initial challenge for our crawler is to identify which links it should follow and explore further. Consider how you would instruct the spider to locate and track these links within a web page.\n",
    "\n",
    "2. **Creating the Spider Class**: To bring our spider to life, we'll start by crafting a Python class aptly named \"Spider.\" This class will serve as the core engine of our web crawler, and its constructor will accept three crucial parameters:\n",
    "   - `starting_url`: The initial URL from which our spider embarks on its journey.\n",
    "   - `crawl_domain`: A domain restriction to ensure that only relevant links are considered for crawling.\n",
    "   - `max_iter`: A limit on the maximum number of web items the spider will collect.\n",
    "\n",
    "3. **Main Method: Spider.run()**: To set our spider in motion, we'll implement a method called `run` within the Spider class. This method will orchestrate the spider's actions, and it's here that we'll outline the core functionalities or building blocks that empower our crawler.\n",
    "\n",
    "Through this project, you'll gain hands-on experience in creating a simplified web spider, providing a foundational understanding of web crawling techniques.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping/Crawling Project Workflow\n",
    "\n",
    "Web scraping and crawling involve a series of steps to access, retrieve, and process web data efficiently and responsibly. The typical workflow for such a project includes the following stages:\n",
    "\n",
    "1. **Accessing the Web (`Acceder web`)**:\n",
    "   - The initial step is to access the target website(s) from which data needs to be scraped.\n",
    "   - This involves sending an HTTP request and receiving the response from the web server.\n",
    "\n",
    "2. **Downloading Web Content (`Bajar web`)**:\n",
    "   - Once access is granted, the next step is to download the content of the webpage.\n",
    "   - This may include HTML, CSS, JavaScript, and media files which make up the webpage.\n",
    "\n",
    "3. **Searching for Links (`Buscar enlaces`)**:\n",
    "   - This step involves parsing the downloaded web content to search for hyperlinks.\n",
    "   - Hyperlinks are identified by the `<a href=\"...\">` HTML tag and are pointers to other webpages.\n",
    "\n",
    "4. **Storing Web Content (`Almacenar web`)**:\n",
    "   - The retrieved web content is then stored locally for processing.\n",
    "   - This storage can be in the form of raw files, or more structured formats like databases.\n",
    "\n",
    "5. **Storing Extracted Links (`Almacenar enlaces`)**:\n",
    "   - Extracted hyperlinks are also stored.\n",
    "   - This forms the basis of the crawling process, where each link can be followed to retrieve more content.\n",
    "\n",
    "6. **Verifying Quality of Links (`Verificar enlaces de saldo`)**:\n",
    "   - Not all links may be relevant or functional.\n",
    "   - This step ensures that the stored links are valid and lead to the necessary content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from urllib.request import urlopen  # For opening URLs\n",
    "from urllib.error import HTTPError  # To handle HTTP errors\n",
    "import time  # To implement delays if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract links from HTML content\n",
    "def getLinks(html, max_links=10):\n",
    "    url = []  # List to store the extracted URLs\n",
    "    cursor = 0  # Cursor to track position in HTML content\n",
    "    nlinks = 0  # Counter for number of links extracted\n",
    "\n",
    "    # Loop to extract links until the maximum is reached or no more links are found\n",
    "    while (cursor >= 0 and nlinks < max_links):\n",
    "        start_link = html.find(\"a href\", cursor)  # Find the start of a link\n",
    "        if start_link == -1:  # If no more links are found, return the list of URLs\n",
    "            return url\n",
    "        start_quote = html.find('\"', start_link)  # Find the opening quote of the URL\n",
    "        end_quote = html.find('\"', start_quote + 1)  # Find the closing quote of the URL\n",
    "        url.append(html[start_quote + 1: end_quote])  # Extract and append the URL to the list\n",
    "        cursor = end_quote + 1  # Move the cursor past this URL\n",
    "        nlinks += 1  # Increment the link counter\n",
    "\n",
    "    return url  # Return the list of URLs\n",
    "\n",
    "# Example usage:\n",
    "# Suppose you have some HTML content stored in a variable `html_content`\n",
    "# You would call the function like this:\n",
    "# links = getLinks(html_content)\n",
    "# This would return a list of URLs extracted from `html_content`\n",
    "\n",
    "# Expected Output:\n",
    "# The output will be a list containing up to `max_links` number of URLs extracted from the given HTML content.\n",
    "# If the HTML content has fewer than `max_links` URLs, all found URLs will be included in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Spider class for web crawling\n",
    "class Spider:\n",
    "    # Initializer or constructor for the Spider class\n",
    "    def __init__(self, starting_url, crawl_domain, max_iter):\n",
    "        self.crawl_domain = crawl_domain  # The domain within which the spider will crawl\n",
    "        self.max_iter = max_iter  # The maximum number of pages to crawl\n",
    "        self.links_to_crawl = [starting_url]  # Queue of links to crawl\n",
    "        self.links_visited = []  # List to keep track of visited links\n",
    "        self.collection = []  # List to store the collected data\n",
    "\n",
    "    # Method to retrieve HTML content from a URL\n",
    "    def retrieveHtml(self):\n",
    "        try:\n",
    "            # Open the URL and read the response\n",
    "            socket = urlopen(self.url)\n",
    "            # Decode the response using 'latin-1' encoding\n",
    "            self.html = socket.read().decode('latin-1')\n",
    "            return 0  # Return 0 if successful\n",
    "        except HTTPError as e:\n",
    "            # If an HTTP error occurs, print the error and return -1\n",
    "            print(f\"HTTP Error encountered: {e}\")\n",
    "            return -1\n",
    "\n",
    "    # Main method to control the crawling process\n",
    "    def run(self):\n",
    "        # Continue to crawl while there are links to crawl and the max_iter is not reached\n",
    "        while self.links_to_crawl and len(self.collection) < self.max_iter:\n",
    "            # Get the next link to crawl\n",
    "            self.url = self.links_to_crawl.pop(0)\n",
    "            print(f\"Currently crawling: {self.url}\")\n",
    "            # Add the link to the list of visited links\n",
    "            self.links_visited.append(self.url)\n",
    "            # If HTML retrieval is successful, store the HTML and find new links\n",
    "            if self.retrieveHtml() >= 0:\n",
    "                self.storeHtml()\n",
    "                self.retrieveAndValidateLinks()\n",
    "\n",
    "    # Method to retrieve and validate links in the HTML content\n",
    "    def retrieveAndValidateLinks(self):\n",
    "        # Get a list of links from the current HTML content\n",
    "        items = getLinks(self.html)\n",
    "        # Temporary list to store valid links\n",
    "        tmpList = []\n",
    "\n",
    "        # Iterate over the found links\n",
    "        for item in items:\n",
    "            item = item.strip('\"')  # Remove any extra quotes\n",
    "        \n",
    "            # Check if the link is an absolute URL that contains the crawl domain\n",
    "            if self.crawl_domain in item and item.startswith('http'):\n",
    "                tmpList.append(item)\n",
    "            # Handle relative links\n",
    "            elif item.startswith('/'):\n",
    "                # Construct the full URL using the crawl domain and relative link\n",
    "                tmpList.append('https://' + self.crawl_domain + item)\n",
    "            # Handle potential relative links without a leading slash (assuming they are not absolute URLs)\n",
    "            elif not item.startswith('http'):\n",
    "                # Construct the full URL assuming it is a relative link\n",
    "                tmpList.append('https://' + self.crawl_domain + '/' + item)\n",
    "\n",
    "        # Add valid, unvisited links to the crawl queue\n",
    "        for item in tmpList:\n",
    "            if item not in self.links_visited and item not in self.links_to_crawl:\n",
    "                self.links_to_crawl.append(item)\n",
    "                print(f'Adding to crawl queue: {item}')\n",
    "\n",
    "\n",
    "    # Method to store HTML content and associated metadata\n",
    "    def storeHtml(self):\n",
    "        # Create a dictionary to represent the document\n",
    "        doc = {\n",
    "            'url': self.url,  # URL of the page\n",
    "            'date': time.strftime(\"%d/%m/%Y\"),  # Current date\n",
    "            'html': self.html  # HTML content of the page\n",
    "        }\n",
    "        # Add the document to the collection\n",
    "        self.collection.append(doc)\n",
    "        print(f\"Stored HTML from: {self.url}\")\n",
    "\n",
    "# Example usage of the Spider class:\n",
    "# Initialize the spider with the starting URL, domain to crawl, and the maximum number of iterations.\n",
    "# my_spider = Spider(\"http://www.example.com\", \"example.com\", 20)\n",
    "\n",
    "# Start the crawling process.\n",
    "# my_spider.run()\n",
    "\n",
    "# After running, my_spider.collection will contain up to 20 pages' HTML from 'example.com'.\n",
    "# Each page's data includes the URL, the date when it was scraped, and the HTML content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of the Spider Class for Web Crawling\n",
    "\n",
    "The `Spider` class is designed for the web crawling process, which systematically browses the web to collect data. Below is an overview of its key functionalities:\n",
    "\n",
    "## Initialization\n",
    "- The class initializes with a `starting_url`, the domain to crawl within (`crawl_domain`), and a maximum number of pages to crawl (`max_iter`).\n",
    "\n",
    "## Crawling Process\n",
    "- The spider maintains a queue of links (`links_to_crawl`) to visit and a list of links already visited (`links_visited`).\n",
    "- The `run` method processes each link in the queue, continuing until the queue is empty or the `max_iter` limit is reached.\n",
    "\n",
    "## HTML Content Retrieval\n",
    "- The `retrieveHtml` method opens each link, reads its content, and decodes it into a string format. It handles success and error cases during this process.\n",
    "\n",
    "## Link Extraction and Validation\n",
    "- `retrieveAndValidateLinks` extracts new links from the current page's HTML, validates them (ensuring they belong to the specified domain), and adds unvisited, valid links to the crawl queue.\n",
    "\n",
    "## Data Storage\n",
    "- The `storeHtml` method saves the HTML content of each visited page, along with the page's URL and the current date, to a collection for later analysis or processing.\n",
    "\n",
    "This class allows for the automated collection of data from a series of web pages within a specific domain, efficiently managing the discovery of new pages to visit based on the links in each page.\n",
    "\n",
    "Let us validate the crawler with the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the Spider class is defined as before with getLinks function properly defined\n",
    "\n",
    "# Example usage of the Spider class:\n",
    "\n",
    "# Instantiate the Spider with the starting URL, the domain to crawl within, and the maximum number of iterations.\n",
    "# The crawl domain is typically the base domain from which the crawler should not deviate.\n",
    "spider = Spider('https://books.toscrape.com/', 'books.toscrape.com', 20)\n",
    "\n",
    "# Start the crawling process.\n",
    "spider.run()\n",
    "\n",
    "# After running, `spider.collection` will contain the HTML content of up to 20 pages from 'ironhack.com'.\n",
    "# Each entry in the collection will include the URL, the date when it was scraped, and the HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many elements does our colletion have?\n",
    "len(spider.collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider.collection[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enumerate the urls retreived\n",
    "[spider.collection[i]['url'] for i in range(len(spider.collection))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the simple crawler more or less works as expected. There are still many functionalities to work on , such as valid domains, valid urls, etc. One important issue to consider is **persistence**, or how to store the data retrieved for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Business Challenge: Extracting Product Data from a Fake Online Store\n",
    "\n",
    "## Objective\n",
    "Your task is to perform a product analysis by collecting data from a simulated online store, such as \"Fake Store API\" (https://fakestoreapi.com/). This website is designed for practice and offers a safe environment for web scraping.\n",
    "\n",
    "## Steps\n",
    "1. **Data Collection**:\n",
    "   - Use the `Spider` class to crawl the \"Fake Store API\" website.\n",
    "   - Collect data on products, including names, categories, prices, and descriptions.\n",
    "\n",
    "2. **Data Storage**:\n",
    "   - Store the scraped data in a structured format, such as a CSV file or a database.\n",
    "\n",
    "3. **Analysis**:\n",
    "   - Analyze the collected data to understand product distribution across different categories, price ranges, and other relevant metrics.\n",
    "\n",
    "4. **Report**:\n",
    "   - Prepare a report summarizing your findings, including insights on product trends, pricing strategies, and category popularity.\n",
    "\n",
    "## Tasks for Students\n",
    "- Work in pairs to plan and execute the web scraping process.\n",
    "- Ensure ethical scraping practices are followed, including adhering to `robots.txt` guidelines and rate limiting requests.\n",
    "- Conduct a thorough analysis of the collected data and collaborate to create a comprehensive report.\n",
    "- Present your findings in class, highlighting key insights and methodologies used.\n",
    "\n",
    "## Expected Outcome\n",
    "Gain hands-on experience in web scraping, data analysis, and presenting findings in a business context. This project will also enhance your understanding of online retail market dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using APIs for Data Retrieval\n",
    "\n",
    "### Understanding the Big Picture\n",
    "\n",
    "When aiming to retrieve specific data from a website, it's crucial to first check if the website offers a programmatic interface for querying data. Such interfaces, known as Application Programming Interfaces (APIs), provide a more efficient and structured way of accessing data compared to web scraping.\n",
    "\n",
    "### The Advantage of APIs\n",
    "\n",
    "APIs, particularly RESTful APIs, offer a well-defined method for interacting with web services. They are built on a set of rules and standards that allow for predictable and straightforward data retrieval. Here's what characterizes a RESTful API:\n",
    "\n",
    "- **Base URI**: Every RESTful API has a base Uniform Resource Identifier (URI), which serves as the entry point for the API. For example, `http://example.com/resources/` could be a base URI.\n",
    "\n",
    "- **Internet Media Type**: RESTful APIs often return data in a specific format, such as JSON (JavaScript Object Notation), which is widely used due to its simplicity and readability. However, other formats like XML, Atom, or even images can be used.\n",
    "\n",
    "- **Standard HTTP Methods**: These APIs leverage standard HTTP methods for operations:\n",
    "    - `GET`: Retrieve data from the server (e.g., a list of products).\n",
    "    - `PUT`: Update existing data or create new data if it doesn't exist, and it's an idempotent operation (repeating the request results in the same state).\n",
    "    - `POST`: Create new data or update existing data (not idempotent).\n",
    "    - `DELETE`: Remove data.\n",
    "\n",
    "- **Hypertext Links for State and Navigation**: RESTful APIs often use hypertext links (URLs) to represent the current state of an application or to navigate between related resources.\n",
    "\n",
    "### Using APIs with Authentication\n",
    "\n",
    "Many RESTful APIs require authentication for security reasons. This is typically done by sending a token or key with your API request, which verifies your identity and authorizes your access to the API. The process of obtaining and using authentication tokens varies between APIs, so it's essential to refer to the specific API's documentation for guidance.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Leveraging APIs for data retrieval not only aligns with ethical web practices but also ensures a more stable and efficient way of accessing data. When an API is available, it's usually the preferred method over web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Fetching Weather Data Using OpenWeatherMap API in Python\n",
    "\n",
    "This example demonstrates how to use the OpenWeatherMap API to fetch current weather data for a specific city using Python.\n",
    "\n",
    "### Prerequisites\n",
    "- An API key from OpenWeatherMap.\n",
    "- Python's `requests` library installed. (Install via `pip install requests` if needed.)\n",
    "\n",
    "### Steps to Follow\n",
    "1. **Sign Up for OpenWeatherMap API**:\n",
    "   - Register for an account at [OpenWeatherMap](https://openweathermap.org/api).\n",
    "   - Obtain your free API key (note that there might be an activation delay).\n",
    "\n",
    "2. **Python Script for Weather Data Retrieval**:\n",
    "   - The script uses the `requests` library to make an API call.\n",
    "   - Replace `'YOUR_API_KEY'` with your actual OpenWeatherMap API key.\n",
    "   - Replace `'CITY_NAME'` with your desired city name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather(api_key, city):\n",
    "    base_url = \"http://api.openweathermap.org/data/2.5/weather?\"\n",
    "    city_name = city\n",
    "    complete_url = f\"{base_url}appid={api_key}&q={city_name}\"\n",
    "    response = requests.get(complete_url)\n",
    "    return response.json()\n",
    "\n",
    "# Replace 'YOUR_API_KEY' with your actual API key and 'CITY_NAME' with your city\n",
    "api_key = 'YOUR_API_KEY'\n",
    "city_name = 'CITY_NAME'\n",
    "weather_data = get_weather(api_key, city_name)\n",
    "\n",
    "print(f\"Weather in {city_name}:\")\n",
    "print(weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "The script will output the current weather data in JSON format, which includes temperature, humidity, weather description, etc.\n",
    "\n",
    "### Note\n",
    "- Ensure you replace `'YOUR_API_KEY'` and `'CITY_NAME'` with your actual API key and desired city.\n",
    "- The OpenWeatherMap API provides data in various formats and details. You might want to explore their documentation for more specific use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Analyzing Instagram Hashtag Trends with Instaloader\n",
    "\n",
    "## Objective\n",
    "Leverage `Instaloader`, a Python library, to download posts associated with a specific hashtag on Instagram. Analyze the collected data to identify trends, popular content, and user engagement.\n",
    "\n",
    "## Steps\n",
    "\n",
    "### 1. Install Instaloader\n",
    "- Ensure Python is installed on your system.\n",
    "- Install `Instaloader` using pip: `pip install instaloader`\n",
    "\n",
    "\n",
    "### 2. Data Collection\n",
    "- Choose a hashtag relevant to a topic of interest (e.g., #nature, #travel, #food).\n",
    "- Use `Instaloader` to download posts tagged with the chosen hashtag. Consider limitations like the number of posts to avoid overwhelming the API.\n",
    "\n",
    "```python\n",
    "import instaloader\n",
    "\n",
    "L = instaloader.Instaloader()\n",
    "posts = instaloader.Hashtag.from_name(L.context, 'YOUR_HASHTAG').get_posts()\n",
    "\n",
    "for post in posts:\n",
    "    # Add code to process and store post details\n",
    "```\n",
    "### 3. Data Analysis\n",
    "Analyze the downloaded data for:\n",
    "- Popular trends in the hashtag.\n",
    "- Common themes or subjects in images or captions.\n",
    "- Levels of user engagement (likes, comments).\n",
    "\n",
    "### 4. Reporting\n",
    "- Compile your findings into a report.\n",
    "- Include visual representations (graphs, word clouds) to illustrate key trends.\n",
    "\n",
    "### Important Notes\n",
    "- Respect Instagram's terms of service and ethical guidelines in data scraping.\n",
    "- Be mindful of privacy and consent, especially with user-generated content.\n",
    "- The scope of data collection should be limited for educational purposes.\n",
    "\n",
    "### Expected Outcome\n",
    "This challenge aims to provide practical experience with Instaloader, develop data analysis skills, and offer insights into social media trends and user behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
