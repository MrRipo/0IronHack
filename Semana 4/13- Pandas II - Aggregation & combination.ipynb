{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b823de73",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-aggregation\" data-toc-modified-id=\"Data-aggregation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data aggregation</a></span><ul class=\"toc-item\"><li><span><a href=\"#groupby\" data-toc-modified-id=\"groupby-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>groupby</a></span><ul class=\"toc-item\"><li><span><a href=\"#groupby-&amp;-aggregate\" data-toc-modified-id=\"groupby-&amp;-aggregate-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>groupby &amp; aggregate</a></span></li></ul></li><li><span><a href=\"#Pivot-Tables\" data-toc-modified-id=\"Pivot-Tables-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Pivot Tables</a></span></li><li><span><a href=\"#Differences-between-pivot-table-and-groupby-function\" data-toc-modified-id=\"Differences-between-pivot-table-and-groupby-function-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Differences between pivot table and groupby function</a></span></li><li><span><a href=\"#crosstab\" data-toc-modified-id=\"crosstab-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>crosstab</a></span></li></ul></li><li><span><a href=\"#Data-combination:-mixing-dataframes\" data-toc-modified-id=\"Data-combination:-mixing-dataframes-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data combination: mixing dataframes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Concat:-two-things-together\" data-toc-modified-id=\"Concat:-two-things-together-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Concat: two things together</a></span><ul class=\"toc-item\"><li><span><a href=\"#Concatenating-on-axis-0-(rows)\" data-toc-modified-id=\"Concatenating-on-axis-0-(rows)-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Concatenating on axis 0 (rows)</a></span></li><li><span><a href=\"#Concatenating-on-axis-1-(columns)\" data-toc-modified-id=\"Concatenating-on-axis-1-(columns)-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Concatenating on axis 1 (columns)</a></span></li></ul></li><li><span><a href=\"#Merge:-related-columns\" data-toc-modified-id=\"Merge:-related-columns-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Merge: related columns</a></span><ul class=\"toc-item\"><li><span><a href=\"#LEFT-Merge\" data-toc-modified-id=\"LEFT-Merge-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>LEFT Merge</a></span></li><li><span><a href=\"#RIGHT-Merge\" data-toc-modified-id=\"RIGHT-Merge-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>RIGHT Merge</a></span></li><li><span><a href=\"#INNER-Merge\" data-toc-modified-id=\"INNER-Merge-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>INNER Merge</a></span></li><li><span><a href=\"#OUTER-Merge\" data-toc-modified-id=\"OUTER-Merge-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>OUTER Merge</a></span></li></ul></li><li><span><a href=\"#Merging-&amp;-Concatenating-in-two-different-columns\" data-toc-modified-id=\"Merging-&amp;-Concatenating-in-two-different-columns-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Merging &amp; Concatenating in two different columns</a></span><ul class=\"toc-item\"><li><span><a href=\"#Concatenating-in-two-different-columns\" data-toc-modified-id=\"Concatenating-in-two-different-columns-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Concatenating in two different columns</a></span></li><li><span><a href=\"#Merging-in-two-different-columns\" data-toc-modified-id=\"Merging-in-two-different-columns-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Merging in two different columns</a></span></li></ul></li><li><span><a href=\"#Join:-relating-index\" data-toc-modified-id=\"Join:-relating-index-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Join: relating index</a></span></li><li><span><a href=\"#Differences-between-join-&amp;-merge\" data-toc-modified-id=\"Differences-between-join-&amp;-merge-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Differences between join &amp; merge</a></span></li></ul></li><li><span><a href=\"#Pandas-usual-methods\" data-toc-modified-id=\"Pandas-usual-methods-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pandas usual methods</a></span></li><li><span><a href=\"#Further-materials\" data-toc-modified-id=\"Further-materials-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Further materials</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d0378",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1400/1*6d5dw6dPhy4vBp2vRW6uzw.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4670f9",
   "metadata": {},
   "source": [
    "First, we import the main libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfec44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af75b7d",
   "metadata": {},
   "source": [
    "Now, is time to load a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998423db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/employees.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2050b9",
   "metadata": {},
   "source": [
    "what is it about? Let's look at it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc1f24d",
   "metadata": {},
   "source": [
    "## Data aggregation\n",
    "\n",
    "Data aggregation is a fundamental technique in data analysis that involves grouping and summarizing data to gain insights and make informed decisions. In this section, we will explore essential Pandas methods for data aggregation, including `groupby`, `pivot_table`, and `crosstab`.\n",
    "\n",
    "These techniques are particularly useful when dealing with large datasets or when you need to summarize data across different categories or dimensions. Whether you want to calculate statistics for specific groups, create summary tables, or analyze relationships between variables, data aggregation methods in Pandas provide the tools to streamline your analysis.\n",
    "\n",
    "Let's dive into each of these methods and discover how they can help you uncover meaningful patterns and trends within your data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51e74f4",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### groupby\n",
    "\n",
    "Pandas GroupBy is a powerful and versatile function in Python that is indispensable for data manipulation and analysis. It enables us to split a DataFrame into separate groups based on one or more criteria and perform various calculations on these groups for in-depth analysis.\n",
    "\n",
    "The concept is straightforward: you select a DataFrame, specify the categories or criteria by which you want to group your data, and then apply aggregation functions to each group. These aggregation functions can include calculations like sum, mean, count, or even custom-defined functions, depending on your analysis goals.\n",
    "\n",
    "GroupBy operations are invaluable for tasks such as summarizing data, exploring patterns within specific groups, or comparing different segments of your dataset. Whether you're working with sales data, customer behavior, or any other structured data, Pandas GroupBy is your go-to tool.\n",
    "\n",
    "Here's a basic syntax to use GroupBy:\n",
    "\n",
    "`df[subset].groupby(category).aggregation()`\n",
    "\n",
    "To dive deeper into the capabilities and options of Pandas GroupBy, you can refer to the official documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c6bb3",
   "metadata": {},
   "source": [
    "`groupby the departments and their mean age`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3877a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two ways of doing it (1st)\n",
    "df[[\"Age\", \"Department\"]].groupby(\"Department\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two ways of doing it (2nd)\n",
    "df[[\"Age\", \"Department\"]].groupby(\"Department\").agg(\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968b6e9",
   "metadata": {},
   "source": [
    "`groupby the departments and their maximum age`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b4e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two ways of doing it (1st)\n",
    "df[[\"Age\", \"Department\"]].groupby(\"Department\").agg({\"Age\":\"max\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62424b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two ways of doing it (2nd)\n",
    "df[[\"Age\", \"Department\"]].groupby(\"Department\").max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28133159",
   "metadata": {},
   "source": [
    "`groupby the departments and their min age`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two ways of doing it (1st)\n",
    "df[[\"Age\", \"Department\"]].groupby(\"Department\").agg({\"Age\":\"min\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a966a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two ways of doing it (2nd)\n",
    "df[[\"Age\", \"Department\"]].groupby(\"Department\").min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08947d4b",
   "metadata": {},
   "source": [
    "`groupby: 2+`: Different Education Fields and their Mean Salaries in each of the Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ddaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"EducationField\", \"HourlyRate\", \"Department\"]].groupby(by=[\"Department\", \"EducationField\"]).agg({\"HourlyRate\":\"mean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad439933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"EducationField\", \"HourlyRate\", \"Department\"]].groupby(by=[\"EducationField\", \"Department\"]).agg({\"HourlyRate\":\"mean\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13345cef",
   "metadata": {},
   "source": [
    "`more fields`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18336af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"EducationField\", \"HourlyRate\", \"Department\", \"StandardHours\", \"Age\"]].groupby(by=[\"Department\", \"EducationField\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b533d07",
   "metadata": {},
   "source": [
    "#### groupby & aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d23b8f4",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "In Pandas, the \"agg\" function is a powerful tool for aggregating data within groups created using the GroupBy operation. It allows you to apply multiple aggregation functions simultaneously, providing concise and flexible control over your data summarization.\n",
    "\n",
    "Here are some common aggregation functions that can be used with the \"agg\" syntax:\n",
    "- **mean**: Calculates the mean (average) of the values in each group.\n",
    "- **sum**: Adds up all the values in each group.\n",
    "- **count**: Counts the number of rows in each group.\n",
    "\n",
    "Using the \"agg\" function, you can specify these aggregation functions and more, tailoring your data summarization to match your specific analysis requirements. This flexibility is especially useful when you need to obtain various statistics from your grouped data simultaneously.\n",
    "\n",
    "For more details and advanced options regarding the \"agg\" function, you can refer to the official Pandas documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4930acad",
   "metadata": {},
   "source": [
    "### Pivot Tables\n",
    "\n",
    "In data analysis, pivot tables are a valuable tool for reshaping and summarizing data, especially when you need to analyze and compare multiple aspects of your dataset simultaneously. Pandas provides a versatile `pivot_table` function that allows you to create spreadsheet-style pivot tables within your DataFrame.\n",
    "\n",
    "The `pivot_table` function takes several key parameters:\n",
    "- `df`: The DataFrame you want to pivot.\n",
    "- `values`: The columns containing the values you want to aggregate.\n",
    "- `index`: The columns that will become the new index of the pivot table.\n",
    "- `columns`: The columns that will become the new columns of the pivot table.\n",
    "- `aggfunc`: The aggregation function to be applied to the values (default is 'mean').\n",
    "\n",
    "When you apply the `pivot_table` function, it organizes your data into a more structured format, with hierarchical indexing on both rows and columns. This structure makes it easy to perform various aggregation operations, such as calculating means, sums, or counts across different groups and dimensions.\n",
    "\n",
    "For a detailed comparison of when to use `groupby` versus `pivot_table`, you can refer to this informative article: [GroupBy vs. Pivot Table](https://towardsdatascience.com/a-comparison-of-groupby-and-pivot-in-pythons-pandas-module-527909e78d6b).\n",
    "\n",
    "Utilizing pivot tables effectively can significantly enhance your data analysis capabilities and help you uncover insights from complex datasets.\n",
    "\n",
    "To learn more about the `pivot_table` function and its advanced options, you can refer to the official Pandas documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7b8eec",
   "metadata": {},
   "source": [
    "`pivot table`: the department and their mean ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f601835",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c5545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example, calculating mean age by department\n",
    "df.pivot_table(\n",
    "    values = \"Age\",\n",
    "    index = \"Department\",\n",
    "    aggfunc = \"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f17fa4b",
   "metadata": {},
   "source": [
    "Now, is your **turn**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e6d7f",
   "metadata": {},
   "source": [
    "`pivot table`: the department and their max ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b920ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23fdbbe2",
   "metadata": {},
   "source": [
    "`pivot table`: the department and their min ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c804160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b08ccf2",
   "metadata": {},
   "source": [
    "`pivot table`: the department and their median age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc473f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6e9e0c6",
   "metadata": {},
   "source": [
    "`multi-index pivot table`: Department and Education Field and check the maximum salary values of employees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example, calculating mean age by department and EducationField\n",
    "\n",
    "df.pivot_table (\n",
    "    values = \"MonthlyIncome\",\n",
    "    index = [\"Department\", \"EducationField\"],\n",
    "    aggfunc = \"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b715869",
   "metadata": {},
   "source": [
    "### crosstab\n",
    "\n",
    "In data analysis, understanding the relationships between categorical variables is often crucial for gaining insights into your dataset. Pandas offers a powerful tool called `crosstab` that helps you compute a simple cross-tabulation of two or more factors within your DataFrame.\n",
    "\n",
    "The `pd.crosstab()` function is designed for this purpose and is particularly useful when you want to examine the frequency or distribution of one categorical variable relative to another. By default, it computes a frequency table, but you can also pass an array of values and an aggregation function for more advanced analysis.\n",
    "\n",
    "Here's the basic syntax:\n",
    "```python\n",
    "pd.crosstab(df[column1], df[column2])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46987cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example I\n",
    "pd.crosstab(df[\"Department\"], df[\"EducationField\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042fbd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example II\n",
    "pd.crosstab(df[\"Department\"], df[\"EducationField\"], df[\"MonthlyIncome\"], aggfunc=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465cda9",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c1560",
   "metadata": {},
   "source": [
    "## Data combination: mixing dataframes\n",
    "\n",
    "In real-world data analysis, you often encounter scenarios where you need to combine data from different sources or organize it differently to perform meaningful analysis. Pandas provides several powerful methods for merging, joining, and concatenating DataFrames to help you manage this process efficiently.\n",
    "\n",
    "For a comprehensive understanding of these techniques, consider exploring the following resources:\n",
    "- [Real Python: Pandas Merge, Join, and Concat](https://realpython.com/pandas-merge-join-and-concat/)\n",
    "- [Pandas Documentation: Merging and Joining](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c60a7d",
   "metadata": {},
   "source": [
    "### Concat: two things together\n",
    "Concatenation is one of the fundamental operations for combining DataFrames. With `pd.concat()`, you can effortlessly join DataFrames along a specified axis, usually along axis 0 (vertically), to stack them one below the other. This aligns columns based on their labels, making it a valuable tool for extending or appending data vertically.\n",
    "\n",
    "The basic syntax for concatenation is as follows:\n",
    "```python\n",
    "result = pd.concat([df1, df2, ...], axis=0)\n",
    "```\n",
    "Here, `df1`, `df2`, and so on are the DataFrames you want to concatenate. The `axis=0` argument ensures that the concatenation occurs vertically, aligning rows.\n",
    "\n",
    "By mastering concatenation, you can efficiently manage datasets that grow over time or combine data from various sources into a single cohesive DataFrame. This is particularly useful when dealing with time series data, log files, or other situations where data is collected incrementally.\n",
    "\n",
    "To delve deeper into concatenation and explore more advanced options, you can refer to the provided resources and the official Pandas documentation on [pd.concat()](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac484cd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load some data\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user',\n",
    "                             sep='|', \n",
    "                             index_col='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63950f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's sample it\n",
    "first_df = df.sample(frac=0.1)\n",
    "second_df = df.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb7c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac7695b",
   "metadata": {},
   "source": [
    "#### Concatenating on axis 0 (rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6af3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([first_df, second_df], axis=0, keys=[\"Table 1\", \"Table 2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80517add",
   "metadata": {},
   "source": [
    "- This operation concatenates the DataFrames `first_df` and `second_df` vertically (along `axis=0`), meaning it stacks them on top of each other.\n",
    "\n",
    "- The `keys` parameter is used to specify a hierarchical index for the resulting DataFrame. In this case, you're giving each original DataFrame a label: \"Table 1\" and \"Table 2.\"\n",
    "\n",
    "- As a result, the output will be a new DataFrame with a hierarchical (MultiIndex) structure. You can think of it as having two sections labeled \"Table 1\" and \"Table 2,\" where each section contains the data from the respective original DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5d00f5",
   "metadata": {},
   "source": [
    "#### Concatenating on axis 1 (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7118492",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([first_df, second_df], axis=1, keys=[\"left\", \"right\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8274d6cf",
   "metadata": {},
   "source": [
    "- This operation concatenates the DataFrames `first_df` and `second_df` horizontally (along `axis=1`), meaning it combines them side by side.\n",
    "\n",
    "- Similar to the first operation, the `keys` parameter is used to label the columns resulting from each original DataFrame. Here, you're labeling them as \"left\" and \" right.\"\n",
    "\n",
    "- The output will be a new DataFrame where the columns are organized into two sections labeled \"left\" and \" right,\" representing the data from the respective original DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd65ff4",
   "metadata": {},
   "source": [
    "![sql joins](https://upload.wikimedia.org/wikipedia/commons/9/9d/SQL_Joins.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-klein",
   "metadata": {},
   "source": [
    "### Merge: related columns\n",
    "\n",
    "The `merge()` function in Pandas is a powerful tool when you need to combine rows from multiple DataFrames based on related columns. It's primarily used for database-style joins where you want to bring together data from different sources that share common key columns.\n",
    "\n",
    "**Understanding the `merge()` Function**:\n",
    "\n",
    "You can think of the `merge()` function as a way to merge rows that share data across specified columns. It allows you to perform various types of joins, such as inner, outer, left, and right joins, which determine how rows from both DataFrames are combined.\n",
    "\n",
    "- **Inner Join**: This type of join returns only the rows where there is a match in both DataFrames based on the specified columns. Rows without a match are excluded.\n",
    "\n",
    "- **Outer Join (Full Outer Join)**: An outer join returns all rows when there is a match in either the left or right DataFrame. Unmatched rows will contain NaN values for the missing data.\n",
    "\n",
    "- **Left Join (Left Outer Join)**: A left join returns all rows from the left DataFrame and the matched rows from the right DataFrame. Unmatched rows from the left DataFrame will still be included.\n",
    "\n",
    "- **Right Join (Right Outer Join)**: Conversely, a right join returns all rows from the right DataFrame and the matched rows from the left DataFrame. Unmatched rows from the right DataFrame will still be included.\n",
    "\n",
    "**Usage and Syntax**:\n",
    "\n",
    "Here's a basic outline of how to use the `merge()` function:\n",
    "\n",
    "```python\n",
    "merged_df = pd.merge(left_df, right_df, on='common_column', how='join_type')\n",
    "```\n",
    "- **Usage and Syntax:**\n",
    "\n",
    "  - `left_df`: The left DataFrame you want to merge.\n",
    "  - `right_df`: The right DataFrame you want to merge.\n",
    "  - `on`: The column(s) on which you want to perform the merge. These columns should exist in both DataFrames and serve as the key(s) for the merge operation.\n",
    "  - `how`: The type of join you want to perform (e.g., 'inner', 'outer', 'left', 'right').\n",
    "\n",
    "- **Benefits of Using `merge()`:**\n",
    "\n",
    "  - **Data Integration:** Merge helps integrate data from multiple sources or tables into a single, comprehensive dataset.\n",
    "\n",
    "  - **Data Analysis:** It simplifies the process of combining related information, making it easier to analyze data and derive insights.\n",
    "\n",
    "  - **Database Operations:** `merge()` aligns with common database operations like SQL joins, allowing data analysts to leverage their SQL knowledge within Pandas.\n",
    "\n",
    "  - **Complex Queries:** You can handle complex data relationships and perform queries that involve multiple tables efficiently.\n",
    "\n",
    "Merging DataFrames using `merge()` is a fundamental operation in data manipulation and analysis, especially when dealing with real-world datasets from various sources. It provides you with the flexibility to control how data is combined and allows you to work with complex data relationships.\n",
    "\n",
    "For more detailed information and options, you can refer to the official Pandas documentation on [`merge()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c38f1",
   "metadata": {},
   "source": [
    "#### LEFT Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d1fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bc5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(\n",
    "    {\n",
    "        \"key1\": [\"K0\", \"K0\", \"K1\", \"K2\"],\n",
    "        \"key2\": [\"K0\", \"K1\", \"K0\", \"K1\"],\n",
    "        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
    "        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame(\n",
    "    {\n",
    "        \"key1\": [\"K0\", \"K1\", \"K1\", \"K2\"],\n",
    "        \"key2\": [\"K0\", \"K0\", \"K0\", \"K0\"],\n",
    "        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "df3 = pd.DataFrame(\n",
    "    {\n",
    "        \"key1\": [\"K90\", \"K1\", \"K1\", \"K2\"],\n",
    "        \"key2\": [\"K0\", \"K0\", \"K5\", \"K0\"],\n",
    "        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "        \"Y\": [\"D0\", \"D70\", \"D2\", \"D5\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0378289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just showing tables\n",
    "df1_styler = df1.style.set_table_attributes(\"style='display:inline'\").set_caption('Left table')\n",
    "df2_styler = df2.style.set_table_attributes(\"style='display:inline'\").set_caption('Right table')\n",
    "display_html(df1_styler._repr_html_() + \" \" + df2_styler._repr_html_(), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05fdd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge, default is inner\n",
    "pd.merge(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793eb3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merges, now is a left\n",
    "pd.merge(df1, df2, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merges, now is a right\n",
    "pd.merge(df1, df2, how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a42620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge, now is inner\n",
    "pd.merge(df1, df2, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade57982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge, now is outer\n",
    "outer_merge = pd.merge(df1, df2, how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce32d1",
   "metadata": {},
   "source": [
    "### Merging & Concatenating in two different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc41cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_docs = pd.DataFrame({'Locations': ['Spain', 'France', 'Portugal', 'Spain'],\n",
    "                    'city': [\"Barcelona\", \"Paris\", \"Lisbon\", \"Madrid\"]})\n",
    "df2_docs = pd.DataFrame({'More locations': ['Spain', 'France', 'Portugal', 'Spain'],\n",
    "                    'city': [\"Madrid\", \"Lyon\", \"Porto\", \"Albacete\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dda9802",
   "metadata": {},
   "source": [
    "#### Concatenating in two different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af74f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1_docs, df2_docs], axis=1, keys=[\"1st table\", \"2nd table\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156a8141",
   "metadata": {},
   "source": [
    "#### Merging in two different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db10f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_docs.merge(df2_docs, left_on='Locations', right_on='More locations', suffixes = [\"_fromlastyear\", \"_fromthisyear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_docs.merge(df2_docs,  left_on='Locations', right_on='More locations', suffixes = [\"_fromleft\", \"_fromright\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e952e34",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Join: relating index\n",
    "The join, unlike the merge, will join the dataframes and where there are no records in the \"index\" it will put NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4660ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd   \n",
    "from IPython.display import display_html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56efa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_df = pd.DataFrame({\"A\": [\"A0\", \"A1\", \"A2\"], \"B\": [\"B0\", \"B1\", \"B2\"]}, index=[\"K0\", \"K1\", \"K2\"])\n",
    "right_df = pd.DataFrame({\"C\": [\"C0\", \"C2\", \"C3\"], \"D\": [\"D0\", \"D2\", \"D3\"]}, index=[\"K0\", \"K2\", \"K3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_df.join(right_df, how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad1415",
   "metadata": {},
   "source": [
    "### Differences between join & merge\n",
    "When combining DataFrames in Pandas, you have two primary methods at your disposal: `join()` and `merge()`. Each method has its own set of features and use cases. Below is a comparison of these two methods based on different join features:\n",
    "\n",
    "| Join Feature               | `join()` | `merge()` |\n",
    "|----------------------------|:--------:|:---------:|\n",
    "| Inner Join                 |   Yes    |    Yes    |\n",
    "| Left Join                  |   Yes    |    Yes    |\n",
    "| Right Join                 |   Yes    |    Yes    |\n",
    "| Outer Join                 |   Yes    |    Yes    |\n",
    "| Cross Join                |    X     |    Yes    |\n",
    "| Join on Indices            |   Yes    |    Yes    |\n",
    "| Join on Columns            |    X     |    Yes    |\n",
    "| Left on Column, Right on Index |   Yes    |    Yes    |\n",
    "| Left on Index, Right on Column |    X     |    Yes    |\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Inner Join:** Both `join()` and `merge()` support inner joins, which return only the rows with matching values in both DataFrames.\n",
    "\n",
    "- **Left Join:** Both methods allow left joins, which include all rows from the left DataFrame and matching rows from the right DataFrame.\n",
    "\n",
    "- **Right Join:** Both methods support right joins, which include all rows from the right DataFrame and matching rows from the left DataFrame.\n",
    "\n",
    "- **Outer Join:** Both methods enable outer joins, which include all rows from both DataFrames, filling in missing values with NaN where necessary.\n",
    "\n",
    "- **Cross Join:** While `merge()` can perform cross joins, `join()` does not support them. Cross joins result in a Cartesian product of rows between two DataFrames.\n",
    "\n",
    "- **Join on Indices:** Both `join()` and `merge()` can perform joins based on the index of DataFrames.\n",
    "\n",
    "- **Join on Columns:** `merge()` allows you to join DataFrames on specific columns, while `join()` does not provide this capability.\n",
    "\n",
    "- **Left on Column, Right on Index:** Both methods support joining on a column from the left DataFrame and the index from the right DataFrame.\n",
    "\n",
    "- **Left on Index, Right on Column:** `merge()` can join on the index from the left DataFrame and a column from the right DataFrame, but `join()` does not offer this option.\n",
    "\n",
    "This comparison should help you choose the appropriate method based on your specific merging requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979ee71",
   "metadata": {},
   "source": [
    "## Business challenge: Glovo orders analysis (use pycharm)\n",
    "\n",
    "In this analysis, we will explore a dataset containing information about orders from Fake partners within the app. Fake partners are stores not directly integrated with Glovo, and our content team manages their product catalog and prices. This dataset provides details on orders, payment discrepancies, and other relevant information.\n",
    "\n",
    "1. Dataset Description\n",
    "    - `order_id`: Unique identifier for each order.\n",
    "    - `activation_time_local`: Local time when the order was activated.\n",
    "    - `country_code`: Country code for the order.\n",
    "    - `store_address`: Address of the store.\n",
    "    - `final_status`: The final status of the order.\n",
    "    - `payment_status`: The payment status of the order.\n",
    "    - `products`: Number of products in the order.\n",
    "    - `products_total`: Total amount at checkout in euros (€).\n",
    "    - `purchase_total_price`: Amount the courier paid at the store in euros (€).\n",
    "\n",
    "2. Background\n",
    "Fake partner orders can exhibit discrepancies between the total amount at checkout (`products_total`) and the amount the courier pays at the store (`purchase_total_price`). When `products_total` is lower than `purchase_total_price`, we classify them as \"under-authorized orders.\" Understanding these discrepancies is crucial for transitioning from charge-on-delivery to an authorize-and-capture model.\n",
    "\n",
    "3. Key Questions\n",
    "During this EDA process, we aim to answer the following questions:\n",
    "\n",
    "    1. What percentage of orders are under-authorized?\n",
    "    2. What percentage of orders would be correctly authorized with a 20% incremental authorization on the amount at checkout?\n",
    "    3. Are there differences in the above percentages when split by country?\n",
    "    4. For the remaining orders outside of incremental authorization, what values would be necessary to capture the remaining amount?\n",
    "    5. Which stores are the most problematic in terms of orders and monetary value?\n",
    "    6. For under-authorized orders, is there a correlation between the price difference and order cancellations? In other words, is an order more likely to be canceled as the price difference increases?\n",
    "\n",
    "Through this analysis, we aim to gain insights into the price fluctuations of past orders and assess the risk associated with transitioning to a new authorization model.\n",
    "\n",
    "Let's dive into the dataset and start exploring!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e6c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"../datasets/fake_orders_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee1311c",
   "metadata": {},
   "source": [
    "# RECAP\n",
    "\n",
    "## Subsets\n",
    "- Select specific columns:\n",
    "  - `df[[col1, col2]]`\n",
    "\n",
    "## Aggregations\n",
    "### Groupby\n",
    "- Perform aggregations based on a categorical column:\n",
    "  - `df[[col1, col2]].groupby(CATEGORICAL).agg({QUANTITATIVE_1: \"mean\", QUANTITATIVE_2: \"max\"})`\n",
    "  - `df[[col1, col2]].groupby(CATEGORICAL).mean()`\n",
    "- Common aggregation functions:\n",
    "  - count, mean, max, min, median, std, and more.\n",
    "\n",
    "### Pivot Table\n",
    "- Create pivot tables to summarize data:\n",
    "  - `df.pivot_table(values=\"MonthlyIncome\", index=\"Department\", aggfunc=\"mean\")`\n",
    "- Pivot tables offer a concise way to aggregate and compare data.\n",
    "\n",
    "### Crosstab\n",
    "- Compute cross-tabulations to analyze the relationship between two categorical variables:\n",
    "  - Default behavior counts occurrences.\n",
    "  - Use `aggfunc=\"mean\"` for aggregating quantitative variables.\n",
    "  - Ensure you select appropriate variables.\n",
    "\n",
    "## JOINS & MERGE & CONCAT\n",
    "\n",
    "### Concatenation\n",
    "- Combining DataFrames physically (not relationally):\n",
    "  - Side by side: `concat` on `axis=1`.\n",
    "    - Use when DataFrames don't share columns or have a common key.\n",
    "  - Top & bottom: `concat` on `axis=0` (appending rows).\n",
    "    - Suitable when DataFrames share columns.\n",
    "\n",
    "### Merge & Join (Pandas)\n",
    "- Merge: Combining DataFrames based on common columns.\n",
    "- Join: Combining DataFrames based on common indices.\n",
    "- In SQL, join operations are performed on columns.\n",
    "\n",
    "These recap points summarize key operations for data manipulation and analysis in Pandas. You can use these techniques to select, aggregate, and combine data efficiently, helping you gain insights from your datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f340806",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Pandas usual methods\n",
    "```python\n",
    "df.head() # prints the head, default 5 rows\n",
    "df.tail() # set the tail, default 5 rows\n",
    "df.describe() # statistical description\n",
    "df.info() # df information\n",
    "df.columns # show column\n",
    "df.index # show index\n",
    "df.dtypes # show column data types\n",
    "df.plot() # make a plot\n",
    "df.hist() # make a histogram\n",
    "df.col.value_counts() # counts the unique values ​​of a column\n",
    "df.col.unique() # returns unique values ​​from a column\n",
    "df.copy() # copies the df\n",
    "df.drop() # remove columns or rows (axis=0,1)\n",
    "df.dropna() # remove nulls\n",
    "df.fillna() # fills nulls\n",
    "df.shape # dimensions of the df\n",
    "df._get_numeric_data() # select numeric columns\n",
    "df.rename() # rename columns\n",
    "df.str.replace() # replace columns of strings\n",
    "df.astype(dtype='float32') # change the data type\n",
    "df.iloc[] # locate by index\n",
    "df.loc[] # locate by element\n",
    "df.transpose() # transposes the df\n",
    "df.T\n",
    "df.sample(n, frac) # sample from df\n",
    "df.col.sum() # sum of a column\n",
    "df.col.max() # maximum of a column\n",
    "df.col.min() # minimum of one column\n",
    "df[col] # select column\n",
    "df.col\n",
    "df.isnull() # null values\n",
    "df.isna()\n",
    "df.notna() # not null values\n",
    "df.drop_duplicates() # remove duplicates\n",
    "df.reset_index(inplace=True) # reset the index and overwrite\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-jefferson",
   "metadata": {},
   "source": [
    "## Further materials\n",
    "\n",
    "* [Read the docs!](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
    "* [Cheatsheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "* [Exercises to practice](https://github.com/guipsamora/pandas_exercises)\n",
    "* [More on merge, concat, and join](https://realpython.com/pandas-merge-join-and-concat/#pandas-join-combining-data-on-a-column-or-index). And [even more!](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "es"
   ],
   "hotkey": "alt-a",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "269.766px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
